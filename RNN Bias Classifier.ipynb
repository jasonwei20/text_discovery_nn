{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Classification of Israeli and Palestinian Narrative Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time, os, pickle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #get rid of warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First, let's look at the data and process it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18798"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dicts_path = \"pickles/vocab_dicts.p\"\n",
    "word2idx, idx2word, word2vec = pickle.load(open(vocab_dicts_path, 'rb'))\n",
    "len(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in training, dev, and test data from text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training: 61705 isr lines loaded and 61705 pal lines loaded\n",
      "dev: 150 isr lines loaded and 150 pal lines loaded\n",
      "test: 150 isr lines loaded and 150 pal lines loaded\n"
     ]
    }
   ],
   "source": [
    "isr_train = open('processed_data/i_train.txt', 'r').readlines()\n",
    "pal_train = open('processed_data/p_train.txt', 'r').readlines() \n",
    "isr_train = isr_train + isr_train\n",
    "isr_train = isr_train[:len(pal_train)] #balance the training set\n",
    "print(\"training:\", len(isr_train), 'isr lines loaded and', len(pal_train), 'pal lines loaded')\n",
    "\n",
    "isr_dev = open('processed_data/i_dev.txt', 'r').readlines()\n",
    "pal_dev = open('processed_data/p_dev.txt', 'r').readlines() \n",
    "print(\"dev:\", len(isr_dev), 'isr lines loaded and', len(pal_dev), 'pal lines loaded')\n",
    "\n",
    "isr_test = open('processed_data/i_test.txt', 'r').readlines()\n",
    "pal_test = open('processed_data/p_test.txt', 'r').readlines() \n",
    "print(\"test:\", len(isr_test), 'isr lines loaded and', len(pal_test), 'pal lines loaded')\n",
    "\n",
    "sentence_length = 50\n",
    "word2vec_len = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, which has 45 words, we use word2vec to create a 45 by 300 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_lines(num_words, word2vec_len, isr_lines, pal_lines, word2vec):\n",
    "    \n",
    "    n_isr = len(isr_lines)\n",
    "    n_pal = len(pal_lines)\n",
    "    x_matrix = np.zeros((n_isr+n_pal, num_words, word2vec_len))\n",
    "    \n",
    "    #add isr lines first\n",
    "    for i, line in enumerate(isr_lines):\n",
    "        words = line[:-1].split(' ')\n",
    "        words = words[:x_matrix.shape[1]]\n",
    "        for j, word in enumerate(words):\n",
    "            if word in word2vec:\n",
    "                x_matrix[i, j, :] = word2vec[word]\n",
    "    \n",
    "    #then add pal lines\n",
    "    for i, line in enumerate(pal_lines):\n",
    "        words = line[:-1].split(' ')\n",
    "        words = words[:x_matrix.shape[1]]\n",
    "        for j, word in enumerate(words):\n",
    "            if word in word2vec:\n",
    "                x_matrix[i+n_isr, j, :] = word2vec[word]\n",
    "    \n",
    "    y_matrix = np.zeros(n_isr+n_pal)\n",
    "    y_matrix[n_isr:] = 1\n",
    "    \n",
    "    return x_matrix, y_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training, validation, and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training matrix shapes: (123410, 50, 300) (123410,)\n",
      "dev matrix shapes: (300, 50, 300) (300,)\n",
      "test matrix shapes: (300, 50, 300) (300,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_matrix_from_lines(sentence_length, word2vec_len, isr_train, pal_train, word2vec)\n",
    "print(\"training matrix shapes:\", x_train.shape, y_train.shape)\n",
    "#x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "x_dev, y_dev = get_matrix_from_lines(sentence_length, word2vec_len, isr_dev, pal_dev, word2vec)\n",
    "print(\"dev matrix shapes:\", x_dev.shape, y_dev.shape)\n",
    "x_test, y_test = get_matrix_from_lines(sentence_length, word2vec_len, isr_test, pal_test, word2vec)\n",
    "print(\"test matrix shapes:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 50, 100)           140400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 200,901\n",
      "Trainable params: 200,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(sentence_length, return_sequences=True), input_shape=(sentence_length, word2vec_len)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(sentence_length, return_sequences=False)))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since there are many augmented versions of each line, AND we use a sliding window in the data generation, the cross-validation accuracy isn't really valid. That's why we have independent dev and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119707 samples, validate on 3703 samples\n",
      "Epoch 1/2\n",
      "119707/119707 [==============================] - 387s 3ms/step - loss: 0.4683 - acc: 0.7686 - val_loss: 0.3627 - val_acc: 0.8499\n",
      "Epoch 2/2\n",
      "119707/119707 [==============================] - 412s 3ms/step - loss: 0.1960 - acc: 0.9211 - val_loss: 0.3980 - val_acc: 0.8399\n",
      "training time :  801.0629768371582\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model.fit(x_train, y_train, batch_size=1024, nb_epoch=2, validation_split=0.03,  shuffle=True)\n",
    "print('training time : ', time.time() - start)\n",
    "model.save('my_model.h5')\n",
    "#model = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on independent development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_acc 0.8066666666666666\n",
      "test_acc 0.83\n"
     ]
    }
   ],
   "source": [
    "def conf_to_pred(y):\n",
    "    y_class = np.zeros(y.shape)\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] < 0.5:\n",
    "            y_class[i] = 0\n",
    "        else:\n",
    "            y_class[i] = 1\n",
    "    return y_class\n",
    "    \n",
    "def get_accuracy(model, x, y):\n",
    "    y_predict = model.predict(x)\n",
    "    y_class = conf_to_pred(y_predict)\n",
    "    return accuracy_score(y, y_class)\n",
    "\n",
    "dev_acc = get_accuracy(model, x_dev, y_dev)\n",
    "print(\"dev_acc\", dev_acc)\n",
    "test_acc = get_accuracy(model, x_test, y_test)\n",
    "print(\"test_acc\", test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make up your own sentence and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.083140954 the terrorists who used its territory as a base from which to launch strikes at israeli soldiers and civilians injuring many israel retaliated by attacking the areas where the attacks the idf retaliation attacks in the village of samua originated hoping to put a stop \n",
      "\n",
      "0.0019222595 shooting incidents between the idf and the arab armies the confrontations resulted mainly from activities initiated by both sides in areas that had been demilitarized as a result of the armistice agreements at first the idf emerged as the weaker party to the conflict one \n",
      "\n",
      "0.26086769 children were expelled from their schools some months after the nazis came to power they organized a book burning event in which books by jewish authors were burned including works by famous writers such as sigmund freud karl marx and albert einstein next to come \n",
      "\n",
      "0.0016804785 the twentieth century cannot be told without reference to the shoah holocaust as its influence on the jews in the land of israel and around the world was and remains paramount jews are moved into the ghettos about three weeks into world war ii poland \n",
      "\n",
      "0.9959372 to the arabs established schools and helped raise the literacy rate among the arab population the percentage of illiterates dropped from percent to percent at the start of the mandatory era approximately students were attending arab public schools whereas in just before the british mandate \n",
      "\n",
      "0.5647356 pay and appointment higher education law aimed to improve education and to make primary education grades compulsory and free labor law issued in and emphasized parity between men and women in pay it also fought discrimination yet the outbreak of the second intifada did not\n",
      "\n",
      "0.9912717 on the students in the primary stage when the pa assumed its responsibilities in it did its utmost to improve the educational process such as holding training workshops for teachers the pa continued to use school curricula from neighboring arab countries such as jordan until\n",
      "\n",
      "0.85995615 population so finding job opportunities for both old unemployed workers and for young workers became a crisis with the outbreak of the second intifada at the end of december the economic conditions in the palestinian territories became more unbearable the transportation industry considerably declined due\n",
      "\n",
      "0.9888243 the green line as a result a considerable number of workers became unemployed this was accompanied by frequent military closures that prevented workers from reaching their workplaces consequently the individual income and palestinian gross national product fell greatly especially since income from work behind the\n",
      "\n",
      "0.94562805 at taba and it was not possible to reach an agreement there due to the existence of a gap regarding all four issues and also because it was not possible to bridge this gap under the sword of the israeli elections and the pressure of\n",
      "\n",
      "0.9934187 to hold a summit conference in sharm el sheikh on october with the participation of the representative of the european union and the un secretary general the conference aimed at ending violence and bringing the parties back to the negotiating table the negotiations failed after\n",
      "\n",
      "0.02798008 to use military force in any confrontation israel s disrespect of signed agreements international treaties and conventions israel s lack of desire to reach a just and comprehensive peace the absolute siding of the united states with israel in every step and in any forum\n",
      "\n",
      "0.8634363 an urgent issue despite the approximation in position over the intifada between the pa and the other national and islamic forces while the pa saw the intifada as a means for improving its negotiation position the other national forces viewed it as a strategic option\n",
      "\n",
      "0.9941421 remain unknown until now al aqsa intifada a result of the oslo era the first intifada introduced an important stage in the history of the national and political struggle of the palestinians since it moved the conflict to the land of conflict it enabled the\n",
      "\n",
      "0.974034 by their inflexibility they complained that the palestinians wanted all the hand that israel had extended to them instead of the palm and some fingers in a similar fashion the israelis believed that sharon s visit to al aqsa mosque was simply a visit and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_sentence(input_sentence, word2vec, num_words, word2vec_len):\n",
    "    \n",
    "    words = input_sentence.split(\" \")\n",
    "    x = np.zeros((1, num_words, word2vec_len))\n",
    "    for j, word in enumerate(words):\n",
    "            if word.lower() in word2vec:\n",
    "                x[0, j, :] = word2vec[word.lower()]\n",
    "    \n",
    "    y_predict = model.predict(x)\n",
    "    return y_predict\n",
    "    \n",
    "isr_examples = ['the terrorists who used its territory as a base from which to launch strikes at israeli soldiers and civilians injuring many israel retaliated by attacking the areas where the attacks the idf retaliation attacks in the village of samua originated hoping to put a stop',\n",
    "                'shooting incidents between the idf and the arab armies the confrontations resulted mainly from activities initiated by both sides in areas that had been demilitarized as a result of the armistice agreements at first the idf emerged as the weaker party to the conflict one',\n",
    "                'children were expelled from their schools some months after the nazis came to power they organized a book burning event in which books by jewish authors were burned including works by famous writers such as sigmund freud karl marx and albert einstein next to come',\n",
    "                'the twentieth century cannot be told without reference to the shoah holocaust as its influence on the jews in the land of israel and around the world was and remains paramount jews are moved into the ghettos about three weeks into world war ii poland', \n",
    "                'to the arabs established schools and helped raise the literacy rate among the arab population the percentage of illiterates dropped from percent to percent at the start of the mandatory era approximately students were attending arab public schools whereas in just before the british mandate']\n",
    "\n",
    "for isr_example in isr_examples:\n",
    "    print(test_sentence(isr_example, word2vec, sentence_length, word2vec_len)[0][0], isr_example, '\\n')\n",
    "    \n",
    "pal_examples = pal_dev[:10]\n",
    "for pal_example in pal_examples:\n",
    "    print(test_sentence(pal_example, word2vec, sentence_length, word2vec_len)[0][0], pal_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
